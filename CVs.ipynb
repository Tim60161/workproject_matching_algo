{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "Ge6kz43VKrk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools !wheel\n",
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "B1suxqbPXMjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bson"
      ],
      "metadata": {
        "id": "1554GkpAgVpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi"
      ],
      "metadata": {
        "id": "lt0UXvSOhiJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k54y8EWJfm2d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pypdf import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"UpdatedResumeDataSet.csv\")"
      ],
      "metadata": {
        "id": "Omt6tsYsJ246"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Resume[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "8PzVFasLKAtN",
        "outputId": "f12e9ee1-8f0a-4d65-bf1a-393293ee64f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \\r\\n\\r\\nData Science Assurance Associate \\r\\n\\r\\nData Science Assurance Associate - Ernst & Young LLP\\r\\nSkill Details \\r\\nJAVASCRIPT- Exprience - 24 months\\r\\njQuery- Exprience - 24 months\\r\\nPython- Exprience - 24 monthsCompany Details \\r\\ncompany - Ernst & Young LLP\\r\\ndescription - Fraud Investigations and Dispute Services   Assurance\\r\\nTECHNOLOGY ASSISTED REVIEW\\r\\nTAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.\\r\\n* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.\\r\\n* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.\\r\\n* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\\r\\n\\r\\nTools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\r\\n\\r\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\r\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\\r\\n* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.\\r\\n* Created customized tableau dashboards for effective reporting and visualizations.\\r\\nCHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.\\r\\n* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.\\r\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\\r\\n\\r\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer\\r\\n\\r\\nINFORMATION GOVERNANCE\\r\\nOrganizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\\r\\n* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\\r\\n* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.\\r\\n* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.\\r\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\r\\n\\r\\nFRAUD ANALYTIC PLATFORM\\r\\nFraud Analytics and investigative platform to review all red flag cases.\\r\\nâ\\x80¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.\\r\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics\\r\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader = PdfReader(\"CV_ENGLISH_2.pdf\")\n",
        "number_of_pages = len(reader.pages)\n",
        "page = reader.pages[0]\n",
        "text = page.extract_text()"
      ],
      "metadata": {
        "id": "Ct4mlyIjKqyx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "EKITQMbeLOls",
        "outputId": "0a7ed9a3-efe6-4663-84b5-7c0dec76904f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"VICTOR  BJORSVIK  \\n \\nLisbon,  Portugal  | +47 917 83 150 | victorbjorsvik@hotmail.com  \\n \\nQUALIFICATIONS   \\n• Leadership  experience  through  serving  as a Leading  Private  in the Royal  Norwegian  Navy  and leading  teams  at Vender  AS \\n• Creative  and perceptive – influenced  by international  travels  and studying  abroad  in Portugal,  USA  and Hungary  \\n• 6 years  of work  experience  in sales  and service, and  recent  work experience  in more  quantitative  subjects  such as \\nprogramming  for 6 months  and basic  accounting  for 2  years  \\n• Analytical  skills  through quantitative  university  courses  ranging  from  linear  algebra  and calculus  to big data  analysis  \\n• Strong  communication  and teamwork skills  acquired  through  engaging with  customers,  clients,  co-workers  and team -mates  \\n \\nEDUCATION   \\n \\nNOVA  SCHOOL  OF BUSINESS  AND  ECONOMICS  Lisbon,  Portugal  \\nMaster  of Science  in Business  Analytics  2023 - Present  \\n• Concentration:  Business  analytics  with a  particular focus  on finance  \\n• Gained  proficiency  with a wide  array  of quantitative  tools  relevant  to data science , analytics and machine learning  \\n• Electives  in several  financial  subjects  such as financial  modelling,  asset  management  and systematic investments  \\n \\nNORWEGIAN  SCHOOL  OF ECONOMICS  (NHH)  Bergen,  Norway  \\nBachelor  of Science  in Economics  and Business  Administration  2019  - 2022  \\n• GPA  4.43/5.0 – Top 8% in my class  of 500 graduating students  \\n• Gained  familiarity  with a wide  range  of economic  and financial  concepts,  including  valuation  methods  in Excel  and Python  \\n• Designed  and implemented  logistic  regression  models  to forecast  customer  churn  probabilities  within  an insurance  company  \\n• Volunteering:  PR for the UKEN  student  festival  in Bergen  \\n \\nEXPERIENCE   \\n \\nTWO AS  Oslo, Norway  \\nRisk Operation s Analyst  Internship , Fraud Investigation and Prevention  2024 - Present  \\n• Played a key role in a dynamic start -up/scale -up environment, contributing to Two's impressive 30% month -on-month growth \\nand supporting its mission to revolutionize B2B payments.  \\n• Collaborated with Data Science team to mitigate credit and fraud risk , and contributed to customer satisfaction  by providing \\nbespoke credit and recourse limits  \\n• Investigated over 650 fraud cases, actively stopping orders of  over 300K NOK and helped prevent over 7,75 million NOK in \\nfraud losses  \\n \\nVENDER  AS Oslo,  Norway  \\nFacility  management,  Compliance  and Web Development   2019  - 2024  \\n• Conducted a wide variety of activities ranging from office services, reception service, carpentry, accounting, web  \\ndevelopment  and more  \\n• Entrusted  with compliance  to the new Norwegian  law of transparency  (Åpenhetsloven , 2022)  \\n• Developed  a webpage  from  scratch through  Flask  with HTML,  CSS and JavaScript  \\n \\nROYAL  NORWEGIAN  NAVY  Bergen , Norway  \\nWeapons  technician and deckhand  aboard KNM  Magnus  Lagabote  2018  - 2019  \\n• Contributed  to NATO's  Trident  Juncture  18 exercise,  a historic  multinational  military  operation in  Norway,  where  I played a  \\nvital role in testing and  enhancing defence  capabilities  alongside 50,000  participants  from  31 nations  \\n• Promoted  to Private  1st class  for my contributions.  The promotion  took place  within  3 months  aboard  the ship,  the earliest  \\npromotion for a private in  the vessel’s  history  \\n \\n ADDITIONAL  INFORMATION   \\n• Technical:  proficiency  in Python - including  Pandas  and SQL  APIs  for Apache  Spark  and Machine  learning  libraries  including  \\nscikit -learn  and TensorFlow , C, R, SQL, HTML, CSS,  Tableau  and more  \\n• Courses: CS50x (Harvard), MIT 6.00.1x (Massachusetts Institute of Technology), Market Concepts (Bloomberg), The  \\nComplete Financial  Analyst Course  (Udemy), Machine learning Specialization (Stanford & Deeplearning.AI)  \\n• Interests:  cooking,  climbing,  hiking,  travelling  \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Resources import DEGREES_IMPORTANCE\n",
        "from spacy.lang.en import English\n",
        "\n",
        "\n",
        "class JobInfoExtraction:\n",
        "\n",
        "    def __init__(self, skills_patterns_path, majors_patterns_path, degrees_patterns_path, jobs):\n",
        "        # self.jobs = jobs[['Qualifications']]\n",
        "        self.skills_patterns_path = skills_patterns_path\n",
        "        self.majors_patterns_path = majors_patterns_path\n",
        "        self.degrees_patterns_path = degrees_patterns_path\n",
        "        self.degrees_importance = DEGREES_IMPORTANCE\n",
        "\n",
        "    @staticmethod\n",
        "    def match_majors_by_spacy(self, job):\n",
        "        nlp = English()\n",
        "        # Add the pattern to the matcher\n",
        "        patterns_path = self.majors_patterns_path\n",
        "        ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "        ruler.from_disk(patterns_path)\n",
        "        # Process some text\n",
        "        doc1 = nlp(job)\n",
        "        acceptable_majors = []\n",
        "        for ent in doc1.ents:\n",
        "            labels_parts = ent.label_.split('|')\n",
        "            if labels_parts[0] == 'MAJOR':\n",
        "                if labels_parts[2].replace('-', ' ') not in acceptable_majors:\n",
        "                    acceptable_majors.append(labels_parts[2].replace('-', ' '))\n",
        "                if labels_parts[2].replace('-', ' ') not in acceptable_majors:\n",
        "                    acceptable_majors.append(labels_parts[2].replace('-', ' '))\n",
        "        return acceptable_majors\n",
        "\n",
        "    @staticmethod\n",
        "    def match_degrees_by_spacy(self, job):\n",
        "        nlp = English()\n",
        "        # Add the pattern to the matcher\n",
        "        patterns_path = self.degrees_patterns_path\n",
        "        ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "        ruler.from_disk(patterns_path)\n",
        "        # Process some text\n",
        "        doc1 = nlp(job)\n",
        "        degree_levels = []\n",
        "        for ent in doc1.ents:\n",
        "            labels_parts = ent.label_.split('|')\n",
        "            if labels_parts[0] == 'DEGREE':\n",
        "                print((ent.text, ent.label_))\n",
        "                if labels_parts[1] not in degree_levels:\n",
        "                    degree_levels.append(labels_parts[1])\n",
        "        return degree_levels\n",
        "\n",
        "    @staticmethod\n",
        "    def match_skills_by_spacy(self, job):\n",
        "        nlp = English()\n",
        "        patterns_path = self.skills_patterns_path\n",
        "        ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "        ruler.from_disk(patterns_path)\n",
        "        # Process some text\n",
        "        doc1 = nlp(job)\n",
        "        job_skills = []\n",
        "        for ent in doc1.ents:\n",
        "            labels_parts = ent.label_.split('|')\n",
        "            if labels_parts[0] == 'SKILL':\n",
        "                print((ent.text, ent.label_))\n",
        "                if labels_parts[1].replace('-', ' ') not in job_skills:\n",
        "                    job_skills.append(labels_parts[1].replace('-', ' '))\n",
        "        return job_skills\n",
        "\n",
        "    @staticmethod\n",
        "    def get_minimum_degree(self, degrees):\n",
        "        \"\"\"get the minimum degree that the candidate has\"\"\"\n",
        "        d = {degree: self.degrees_importance[degree] for degree in degrees}\n",
        "        return min(d, key=d.get)\n",
        "\n",
        "    def extract_entities(self, jobs):\n",
        "        # recognize and extract entities\n",
        "        jobs['Minimum degree level'] = \"\"\n",
        "        jobs['Acceptable majors'] = \"\"\n",
        "        jobs['Skills'] = \"\"\n",
        "        for i, row in jobs.iterrows():\n",
        "            job = jobs['raw'][i].replace('. ', ' ')\n",
        "            degrees = self.match_degrees_by_spacy(self, job)\n",
        "            if len(degrees) != 0:\n",
        "                jobs['Minimum degree level'][i] = self.get_minimum_degree(self, degrees)\n",
        "            else:\n",
        "                jobs['Minimum degree level'][i] = \"\"\n",
        "            jobs['Acceptable majors'][i] = self.match_majors_by_spacy(self, job)\n",
        "            jobs['Skills'][i] = self.match_skills_by_spacy(self, job)\n",
        "        return jobs\n"
      ],
      "metadata": {
        "id": "y2yGQZnLLYrX"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from bson import ObjectId\n",
        "\n",
        "class PyObjectId(ObjectId):\n",
        "    @classmethod\n",
        "    def __get_validators__(cls):\n",
        "        yield cls.validate\n",
        "\n",
        "    @classmethod\n",
        "    def validate(cls, v):\n",
        "        if not ObjectId.is_valid(v):\n",
        "            raise ValueError(\"Invalid objectid\")\n",
        "        return ObjectId(v)\n",
        "\n",
        "    @classmethod\n",
        "    def __get_pydantic_json_schema__(cls, schema, _field):\n",
        "        schema.update(type=\"string\")\n",
        "        return schema\n",
        "\n",
        "class JobExtractedModel(BaseModel):\n",
        "    id: PyObjectId = Field(default_factory=PyObjectId, alias=\"_id\")\n",
        "    minimum_degree_level: str = Field(...)\n",
        "    acceptable_majors: list = Field(...)\n",
        "    skills: list = Field(...)\n",
        "\n",
        "    class Config:\n",
        "        populate_by_name = True\n",
        "        arbitrary_types_allowed = True\n",
        "        json_encoders = {ObjectId: str}\n"
      ],
      "metadata": {
        "id": "Q4wy6sU5gNyn"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import ast\n",
        "from fastapi import FastAPI\n",
        "from fastapi.encoders import jsonable_encoder\n",
        "\n",
        "def transform_dataframe_to_json(dataframe):\n",
        "\n",
        "    # transforms the dataframe into json\n",
        "    result = dataframe.to_json(orient=\"records\")\n",
        "    parsed = json.loads(result)\n",
        "    json_data = json.dumps(parsed, indent=4)\n",
        "\n",
        "    return json_data\n",
        "\n",
        "\n",
        "def extraction(resume):\n",
        "    degrees_patterns_path = 'Resources/data/degrees.jsonl'\n",
        "    majors_patterns_path = 'Resources/data/majors.jsonl'\n",
        "    skills_patterns_path = 'Resources/data/skills.jsonl'\n",
        "    jobs = resume\n",
        "    job_extraction = JobInfoExtraction(skills_patterns_path, majors_patterns_path, degrees_patterns_path, jobs)\n",
        "    jobs = job_extraction.extract_entities(jobs)\n",
        "    for i, row in jobs.iterrows():\n",
        "        minimum_degree_level = jobs['Minimum degree level'][i]\n",
        "        acceptable_majors = jobs['Acceptable majors'][i]\n",
        "        skills = jobs['Skills'][i]\n",
        "\n",
        "        job_extracted = JobExtractedModel(minimum_degree_level=minimum_degree_level if minimum_degree_level else '',\n",
        "                                          acceptable_majors=acceptable_majors if acceptable_majors else [],\n",
        "                                          skills=skills if skills else [])\n",
        "        job_extracted = jsonable_encoder(job_extracted)\n",
        "        # new_job_extracted = database.get_collection(\"jobsextracted\").insert_one(job_extracted)\n",
        "    jobs_json = transform_dataframe_to_json(jobs)\n",
        "    return jobs_json\n",
        "\n",
        "\n",
        "res = extraction(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vemcLQvUXf47",
        "outputId": "38ad4152-3a09-4057-9e77-b79e32bb17c7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('university', 'DEGREE|BS-LEVEL')\n",
            "('Master', 'DEGREE|MS-LEVEL')\n",
            "('Bachelor', 'DEGREE|BS-LEVEL')\n",
            "('accounting', 'SKILL|accounting')\n",
            "('big data', 'SKILL|big-data')\n",
            "('communication', 'SKILL|communications')\n",
            "('BUSINESS', 'SKILL|business')\n",
            "('Business', 'SKILL|business')\n",
            "('Analytics', 'SKILL|analytics')\n",
            "('Business', 'SKILL|business')\n",
            "('analytics', 'SKILL|analytics')\n",
            "('finance', 'SKILL|finance')\n",
            "('data science', 'SKILL|data-science')\n",
            "('analytics', 'SKILL|analytics')\n",
            "('machine learning', 'SKILL|machine-learning')\n",
            "('modelling', 'SKILL|modelling')\n",
            "('Business', 'SKILL|business')\n",
            "('Python', 'SKILL|python')\n",
            "('models', 'SKILL|modelling')\n",
            "('payments', 'SKILL|payment-services')\n",
            "('Data Science', 'SKILL|data-science')\n",
            "('Web Development', 'SKILL|web-development')\n",
            "('accounting', 'SKILL|accounting')\n",
            "('Flask', 'SKILL|flask')\n",
            "('HTML', 'SKILL|html')\n",
            "('CSS', 'SKILL|css')\n",
            "('JavaScript', 'SKILL|javascript')\n",
            "('testing', 'SKILL|testing')\n",
            "('Python', 'SKILL|python')\n",
            "('Pandas', 'SKILL|pandas')\n",
            "('SQL', 'SKILL|sql')\n",
            "('Spark', 'SKILL|apache-spark')\n",
            "('TensorFlow', 'SKILL|tensorflow')\n",
            "('C', 'SKILL|c')\n",
            "('R', 'SKILL|r')\n",
            "('SQL', 'SKILL|sql')\n",
            "('HTML', 'SKILL|html')\n",
            "('CSS', 'SKILL|css')\n",
            "('Tableau', 'SKILL|tableau')\n",
            "('Machine learning', 'SKILL|machine-learning')\n",
            "('AI', 'SKILL|artificial-intelligence')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Sr9Q6Yjio_a",
        "outputId": "c43e63c3-9fa5-417f-b1da-82dd454a8a31"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[\\n    {\\n        \"raw\": \"VICTOR  BJORSVIK  \\\\n \\\\nLisbon,  Portugal  | +47 917 83 150 | victorbjorsvik@hotmail.com  \\\\n \\\\nQUALIFICATIONS   \\\\n\\\\u2022 Leadership  experience  through  serving  as a Leading  Private  in the Royal  Norwegian  Navy  and leading  teams  at Vender  AS \\\\n\\\\u2022 Creative  and perceptive \\\\u2013 influenced  by international  travels  and studying  abroad  in Portugal,  USA  and Hungary  \\\\n\\\\u2022 6 years  of work  experience  in sales  and service, and  recent  work experience  in more  quantitative  subjects  such as \\\\nprogramming  for 6 months  and basic  accounting  for 2  years  \\\\n\\\\u2022 Analytical  skills  through quantitative  university  courses  ranging  from  linear  algebra  and calculus  to big data  analysis  \\\\n\\\\u2022 Strong  communication  and teamwork skills  acquired  through  engaging with  customers,  clients,  co-workers  and team -mates  \\\\n \\\\nEDUCATION   \\\\n \\\\nNOVA  SCHOOL  OF BUSINESS  AND  ECONOMICS  Lisbon,  Portugal  \\\\nMaster  of Science  in Business  Analytics  2023 - Present  \\\\n\\\\u2022 Concentration:  Business  analytics  with a  particular focus  on finance  \\\\n\\\\u2022 Gained  proficiency  with a wide  array  of quantitative  tools  relevant  to data science , analytics and machine learning  \\\\n\\\\u2022 Electives  in several  financial  subjects  such as financial  modelling,  asset  management  and systematic investments  \\\\n \\\\nNORWEGIAN  SCHOOL  OF ECONOMICS  (NHH)  Bergen,  Norway  \\\\nBachelor  of Science  in Economics  and Business  Administration  2019  - 2022  \\\\n\\\\u2022 GPA  4.43/5.0 \\\\u2013 Top 8% in my class  of 500 graduating students  \\\\n\\\\u2022 Gained  familiarity  with a wide  range  of economic  and financial  concepts,  including  valuation  methods  in Excel  and Python  \\\\n\\\\u2022 Designed  and implemented  logistic  regression  models  to forecast  customer  churn  probabilities  within  an insurance  company  \\\\n\\\\u2022 Volunteering:  PR for the UKEN  student  festival  in Bergen  \\\\n \\\\nEXPERIENCE   \\\\n \\\\nTWO AS  Oslo, Norway  \\\\nRisk Operation s Analyst  Internship , Fraud Investigation and Prevention  2024 - Present  \\\\n\\\\u2022 Played a key role in a dynamic start -up/scale -up environment, contributing to Two\\'s impressive 30% month -on-month growth \\\\nand supporting its mission to revolutionize B2B payments.  \\\\n\\\\u2022 Collaborated with Data Science team to mitigate credit and fraud risk , and contributed to customer satisfaction  by providing \\\\nbespoke credit and recourse limits  \\\\n\\\\u2022 Investigated over 650 fraud cases, actively stopping orders of  over 300K NOK and helped prevent over 7,75 million NOK in \\\\nfraud losses  \\\\n \\\\nVENDER  AS Oslo,  Norway  \\\\nFacility  management,  Compliance  and Web Development   2019  - 2024  \\\\n\\\\u2022 Conducted a wide variety of activities ranging from office services, reception service, carpentry, accounting, web  \\\\ndevelopment  and more  \\\\n\\\\u2022 Entrusted  with compliance  to the new Norwegian  law of transparency  (\\\\u00c5penhetsloven , 2022)  \\\\n\\\\u2022 Developed  a webpage  from  scratch through  Flask  with HTML,  CSS and JavaScript  \\\\n \\\\nROYAL  NORWEGIAN  NAVY  Bergen , Norway  \\\\nWeapons  technician and deckhand  aboard KNM  Magnus  Lagabote  2018  - 2019  \\\\n\\\\u2022 Contributed  to NATO\\'s  Trident  Juncture  18 exercise,  a historic  multinational  military  operation in  Norway,  where  I played a  \\\\nvital role in testing and  enhancing defence  capabilities  alongside 50,000  participants  from  31 nations  \\\\n\\\\u2022 Promoted  to Private  1st class  for my contributions.  The promotion  took place  within  3 months  aboard  the ship,  the earliest  \\\\npromotion for a private in  the vessel\\\\u2019s  history  \\\\n \\\\n ADDITIONAL  INFORMATION   \\\\n\\\\u2022 Technical:  proficiency  in Python - including  Pandas  and SQL  APIs  for Apache  Spark  and Machine  learning  libraries  including  \\\\nscikit -learn  and TensorFlow , C, R, SQL, HTML, CSS,  Tableau  and more  \\\\n\\\\u2022 Courses: CS50x (Harvard), MIT 6.00.1x (Massachusetts Institute of Technology), Market Concepts (Bloomberg), The  \\\\nComplete Financial  Analyst Course  (Udemy), Machine learning Specialization (Stanford & Deeplearning.AI)  \\\\n\\\\u2022 Interests:  cooking,  climbing,  hiking,  travelling  \",\\n        \"Minimum degree level\": \"BS-LEVEL\",\\n        \"Acceptable majors\": [\\n            \"programming\",\\n            \"data science\",\\n            \"web development\",\\n            \"artificial intelligence\"\\n        ],\\n        \"Skills\": [\\n            \"accounting\",\\n            \"big data\",\\n            \"communications\",\\n            \"business\",\\n            \"analytics\",\\n            \"finance\",\\n            \"data science\",\\n            \"machine learning\",\\n            \"modelling\",\\n            \"python\",\\n            \"payment services\",\\n            \"web development\",\\n            \"flask\",\\n            \"html\",\\n            \"css\",\\n            \"javascript\",\\n            \"testing\",\\n            \"pandas\",\\n            \"sql\",\\n            \"apache spark\",\\n            \"tensorflow\",\\n            \"c\",\\n            \"r\",\\n            \"tableau\",\\n            \"artificial intelligence\"\\n        ]\\n    }\\n]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {\"raw\": [text]}\n",
        "df2 = pd.DataFrame(dic)"
      ],
      "metadata": {
        "id": "RUizl-_tZO-p"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.iloc[0].Skills"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL3M44emcQsk",
        "outputId": "73d963cb-69e7-4cc3-82f2-d92a4f5c6615"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['accounting',\n",
              " 'big data',\n",
              " 'communications',\n",
              " 'business',\n",
              " 'analytics',\n",
              " 'finance',\n",
              " 'data science',\n",
              " 'machine learning',\n",
              " 'modelling',\n",
              " 'python',\n",
              " 'payment services',\n",
              " 'web development',\n",
              " 'flask',\n",
              " 'html',\n",
              " 'css',\n",
              " 'javascript',\n",
              " 'testing',\n",
              " 'pandas',\n",
              " 'sql',\n",
              " 'apache spark',\n",
              " 'tensorflow',\n",
              " 'c',\n",
              " 'r',\n",
              " 'tableau',\n",
              " 'artificial intelligence']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8mzfdOWg9Zq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}