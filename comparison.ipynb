{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\envs\\workproject\\Lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n",
      "c:\\Users\\victo\\anaconda3\\envs\\workproject\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't read file: workproject_matching_algo\\Resources\\data\\degrees.jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 119\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Create DataFrame for resumes\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     df_resumes \u001b[38;5;241m=\u001b[39m get_resumes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresumes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 119\u001b[0m     df_resumes \u001b[38;5;241m=\u001b[39m resume_extraction(df_resumes)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# print(df_resumes[[\"name\", \"Skills\"]])\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# Create DataFrame for jobs\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     description_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkproject_matching_algo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_descriptions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m, in \u001b[0;36mresume_extraction\u001b[1;34m(resumes)\u001b[0m\n\u001b[0;32m     59\u001b[0m names \u001b[38;5;241m=\u001b[39m resumes[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     60\u001b[0m resume_extraction \u001b[38;5;241m=\u001b[39m ResumeInfoExtraction(skills_patterns_path, majors_patterns_path, degrees_patterns_path, resumes, names)\n\u001b[1;32m---> 61\u001b[0m resumes_df \u001b[38;5;241m=\u001b[39m resume_extraction\u001b[38;5;241m.\u001b[39mextract_entities(resumes)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resumes_df\n",
      "File \u001b[1;32mc:\\Users\\victo\\OneDrive - Nova SBE\\WORKPROJECT\\workproject_matching_algo\\services\\ResumeInfoExtraction.py:82\u001b[0m, in \u001b[0;36mResumeInfoExtraction.extract_entities\u001b[1;34m(self, jobs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m jobs\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     81\u001b[0m     job \u001b[38;5;241m=\u001b[39m jobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m     degrees \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch_degrees_by_spacy(\u001b[38;5;28mself\u001b[39m, job)\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(degrees) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     84\u001b[0m         jobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaximum degree level\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_maximum_degree(\u001b[38;5;28mself\u001b[39m, degrees)\n",
      "File \u001b[1;32mc:\\Users\\victo\\OneDrive - Nova SBE\\WORKPROJECT\\workproject_matching_algo\\services\\ResumeInfoExtraction.py:38\u001b[0m, in \u001b[0;36mResumeInfoExtraction.match_degrees_by_spacy\u001b[1;34m(self, job, display)\u001b[0m\n\u001b[0;32m     36\u001b[0m patterns_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegrees_patterns_path\n\u001b[0;32m     37\u001b[0m ruler \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity_ruler\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m ruler\u001b[38;5;241m.\u001b[39mfrom_disk(patterns_path)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Process some text\u001b[39;00m\n\u001b[0;32m     40\u001b[0m doc1 \u001b[38;5;241m=\u001b[39m nlp(job)\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\workproject\\Lib\\site-packages\\spacy\\pipeline\\entityruler.py:489\u001b[0m, in \u001b[0;36mEntityRuler.from_disk\u001b[1;34m(self, path, exclude)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file:\n\u001b[0;32m    488\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mread_jsonl(path)\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_patterns(patterns)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1023\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mpath))\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\workproject\\Lib\\site-packages\\spacy\\pipeline\\entityruler.py:312\u001b[0m, in \u001b[0;36mEntityRuler.add_patterns\u001b[1;34m(self, patterns)\u001b[0m\n\u001b[0;32m    310\u001b[0m phrase_pattern_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    311\u001b[0m phrase_pattern_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    314\u001b[0m         phrase_pattern_labels\u001b[38;5;241m.\u001b[39mappend(entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\workproject\\Lib\\site-packages\\srsly\\_json_api.py:146\u001b[0m, in \u001b[0;36mread_jsonl\u001b[1;34m(path, skip)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m force_path(path)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m _yield_json_lines(f, skip\u001b[38;5;241m=\u001b[39mskip):\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\workproject\\Lib\\site-packages\\srsly\\util.py:24\u001b[0m, in \u001b[0;36mforce_path\u001b[1;34m(location, require_exists)\u001b[0m\n\u001b[0;32m     22\u001b[0m     location \u001b[38;5;241m=\u001b[39m Path(location)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_exists \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m location\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt read file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m location\n",
      "\u001b[1;31mValueError\u001b[0m: Can't read file: workproject_matching_algo\\Resources\\data\\degrees.jsonl"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from services.ResumeInfoExtraction import ResumeInfoExtraction\n",
    "from services.JobInfoExtraction import JobInfoExtraction\n",
    "from source.schemas.resumeextracted import ResumeExtractedModel # Let's reintroduce later on\n",
    "from source.schemas.jobextracted import JobExtractedModel # Let's reintroduce later on\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import openai\n",
    "import warnings \n",
    "import logging\n",
    "import os\n",
    "logging.getLogger('pypdf').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get the absolute path of the root directory\n",
    "ROOT_DIR = (\".\")\n",
    "\n",
    "# Paths to your pattern files\n",
    "degrees_patterns_path = os.path.join(ROOT_DIR, 'Resources', 'data', 'degrees.jsonl')\n",
    "majors_patterns_path = os.path.join(ROOT_DIR,  'Resources', 'data', 'majors.jsonl')\n",
    "skills_patterns_path = os.path.join(ROOT_DIR, 'Resources', 'data', 'skills.jsonl')\n",
    "\n",
    "\n",
    "\n",
    "def get_resumes(directory):\n",
    "    \n",
    "    def extract_pdf(path):\n",
    "        reader = PdfReader(path)\n",
    "        number_of_pages = len(reader.pages)\n",
    "        text = \"\"\n",
    "        for i in range(number_of_pages):\n",
    "            page = reader.pages[i]\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        if os.path.isfile(file_path) and filename.endswith(\".pdf\"):\n",
    "            name = filename.strip(\".pdf\")\n",
    "            resume_text = extract_pdf(file_path)\n",
    "            dic[name] = [resume_text]\n",
    "    \n",
    "    df = pd.DataFrame(dic).T\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={\"index\": \"name\", 0:\"raw\"}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def resume_extraction(resumes):\n",
    "    resumes = resumes.copy()\n",
    "    names = resumes[[\"name\"]]\n",
    "    resume_extraction = ResumeInfoExtraction(skills_patterns_path, majors_patterns_path, degrees_patterns_path, resumes, names)\n",
    "    resumes_df = resume_extraction.extract_entities(resumes)\n",
    "    return resumes_df\n",
    "\n",
    "\n",
    "def job_info_extraction(jobs):\n",
    "    jobs = jobs.copy()\n",
    "    job_extraction = JobInfoExtraction(skills_patterns_path, majors_patterns_path, degrees_patterns_path, jobs)\n",
    "    job_df = job_extraction.extract_entities(jobs)\n",
    "    return job_df\n",
    "\n",
    "\n",
    "def calc_similarity(applicant_df, job_df):\n",
    "    \"\"\"\"Calculate cosine simlarity based on BERT embeddings of skills\"\"\"\n",
    "\n",
    "    def semantic_similarity_sbert_base_v2(job,resume):\n",
    "        \"\"\"calculate similarity with SBERT all-mpnet-base-v2\"\"\"\n",
    "        model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        model.eval()\n",
    "        #Encoding:\n",
    "        score = 0\n",
    "        sen = job+resume\n",
    "        sen_embeddings = model.encode(sen)\n",
    "        for i in range(len(job)):\n",
    "            if job[i] in resume:\n",
    "                score += 1\n",
    "            else:\n",
    "                max_cosine_sim = max(cosine_similarity([sen_embeddings[i]],sen_embeddings[len(job):])[0]) \n",
    "                if max_cosine_sim >= 0.4:\n",
    "                    score += max_cosine_sim\n",
    "        score = score/len(job)  \n",
    "        return round(score,3)\n",
    "    \n",
    "    columns = ['applicant', 'job_id', 'all-mpnet-base-v2_score']\n",
    "    matching_dataframe = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for job_index in range(job_df.shape[0]):\n",
    "        columns = ['applicant', 'job_id', 'all-mpnet-base-v2_score']\n",
    "        matching_dataframe = pd.DataFrame(columns=columns)\n",
    "        ranking_dataframe = pd.DataFrame(columns=columns)\n",
    "        \n",
    "        matching_data = []\n",
    "        \n",
    "        for applicant_id in range(applicant_df.shape[0]):\n",
    "            matching_dataframe_job = {\n",
    "                \"applicant\": applicant_df.iloc[applicant_id, 0],\n",
    "                \"job_id\": job_index,\n",
    "                \"all-mpnet-base-v2_score\": semantic_similarity_sbert_base_v2(job_df['Skills'][job_index], applicant_df['Skills'][applicant_id])\n",
    "            }\n",
    "            matching_data.append(matching_dataframe_job)\n",
    "        \n",
    "        matching_dataframe = pd.concat([matching_dataframe, pd.DataFrame(matching_data)], ignore_index=True)\n",
    "    matching_dataframe['rank'] = matching_dataframe['all-mpnet-base-v2_score'].rank(ascending=False)\n",
    "    return matching_dataframe\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create DataFrame for resumes\n",
    "    df_resumes = get_resumes(\"resumes\")\n",
    "    df_resumes = resume_extraction(df_resumes)\n",
    "    # print(df_resumes[[\"name\", \"Skills\"]])\n",
    "\n",
    "    # Create DataFrame for jobs\n",
    "    description_file_path = os.path.join(ROOT_DIR, 'job_descriptions', 'description.txt')\n",
    "    with open(description_file_path, 'r') as file:\n",
    "        job_description = file.read()\n",
    "\n",
    "    df_jobs = pd.DataFrame([job_description], columns=[\"raw\"])\n",
    "    df_jobs = job_info_extraction(df_jobs)\n",
    "    # print(df_jobs)\n",
    "\n",
    "    analysis_data_df = calc_similarity(df_resumes, df_jobs)\n",
    "    # print(analysis_data_df.sort_values(\"rank\", ascending=True))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workproject",
   "language": "python",
   "name": "workproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
