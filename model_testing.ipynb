{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f52cd73-7a47-4bf5-9419-f9d1f06a5abb",
   "metadata": {},
   "source": [
    "## Model Improvement Suggentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde37613-4048-4edc-8f28-6cf0c426a109",
   "metadata": {},
   "source": [
    "**1:** Test different methods for each model pipeline steps\n",
    "\n",
    "**1.1:** Try different Model\n",
    "- Current Model: all-mpnet-base-v2\n",
    "- [Hugging Face - Model leaderboard:](https://huggingface.co/spaces/mteb/leaderboard) (Try at least 3 best ones)\n",
    "\n",
    "**1.2:** Try skill-by-skill vs. bag-of-skills\n",
    "\n",
    "**1.3:** ...\n",
    "\n",
    "**2:** Last Step: Cross-Encoder without and with special tokens (new approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bf7c9-ee0a-47f6-9ec9-2f62f00cbde4",
   "metadata": {},
   "source": [
    "# 1 Running the Analysis on the Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702b3f1-e75c-4b50-8472-a325a98f7de8",
   "metadata": {},
   "source": [
    "## 1.1 Import the Synthetic Dataset (100 JDs a 6 CVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecce40f8-dd66-4e78-9443-b18a3bb7c1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JD_ID</th>\n",
       "      <th>JD_title</th>\n",
       "      <th>JD_text</th>\n",
       "      <th>CV_ID</th>\n",
       "      <th>CV_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Senior Site Reliability Engineer</td>\n",
       "      <td>About the Team Workday is building a new SRE t...</td>\n",
       "      <td>11</td>\n",
       "      <td>**John Doe**  \\n123 Tech Lane  \\nPleasanton, C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Senior Site Reliability Engineer</td>\n",
       "      <td>About the Team Workday is building a new SRE t...</td>\n",
       "      <td>12</td>\n",
       "      <td>**John Smith**  \\n[LinkedIn Profile] | [Github...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Senior Site Reliability Engineer</td>\n",
       "      <td>About the Team Workday is building a new SRE t...</td>\n",
       "      <td>13</td>\n",
       "      <td>**John D. Anderson**  \\n123 Tech Lane  \\nSan F...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Senior Site Reliability Engineer</td>\n",
       "      <td>About the Team Workday is building a new SRE t...</td>\n",
       "      <td>14</td>\n",
       "      <td>**John Anderson**  \\n123 Tech Avenue  \\nPleasa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Senior Site Reliability Engineer</td>\n",
       "      <td>About the Team Workday is building a new SRE t...</td>\n",
       "      <td>15</td>\n",
       "      <td>**John Smith**  \\n123 Tech Lane  \\nSan Francis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>100</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Why APCON? At APCON, we are committed to setti...</td>\n",
       "      <td>1002</td>\n",
       "      <td>**John Smith**  \\n123 Tech Drive  \\nSan Jose, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>100</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Why APCON? At APCON, we are committed to setti...</td>\n",
       "      <td>1003</td>\n",
       "      <td>**Resume**\\n\\n**John Doe**  \\n123 Main Street ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>100</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Why APCON? At APCON, we are committed to setti...</td>\n",
       "      <td>1004</td>\n",
       "      <td>**John Doe**  \\n[Your Address]  \\n[City, State...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>100</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Why APCON? At APCON, we are committed to setti...</td>\n",
       "      <td>1005</td>\n",
       "      <td>**John Doe**  \\n1234 Elm Street  \\nCityville, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>100</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Why APCON? At APCON, we are committed to setti...</td>\n",
       "      <td>1006</td>\n",
       "      <td>**Emma Johnson**  \\n123 Maple Street  \\nAustin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     JD_ID                          JD_title  \\\n",
       "0        1  Senior Site Reliability Engineer   \n",
       "1        1  Senior Site Reliability Engineer   \n",
       "2        1  Senior Site Reliability Engineer   \n",
       "3        1  Senior Site Reliability Engineer   \n",
       "4        1  Senior Site Reliability Engineer   \n",
       "..     ...                               ...   \n",
       "595    100                 Software Engineer   \n",
       "596    100                 Software Engineer   \n",
       "597    100                 Software Engineer   \n",
       "598    100                 Software Engineer   \n",
       "599    100                 Software Engineer   \n",
       "\n",
       "                                               JD_text  CV_ID  \\\n",
       "0    About the Team Workday is building a new SRE t...     11   \n",
       "1    About the Team Workday is building a new SRE t...     12   \n",
       "2    About the Team Workday is building a new SRE t...     13   \n",
       "3    About the Team Workday is building a new SRE t...     14   \n",
       "4    About the Team Workday is building a new SRE t...     15   \n",
       "..                                                 ...    ...   \n",
       "595  Why APCON? At APCON, we are committed to setti...   1002   \n",
       "596  Why APCON? At APCON, we are committed to setti...   1003   \n",
       "597  Why APCON? At APCON, we are committed to setti...   1004   \n",
       "598  Why APCON? At APCON, we are committed to setti...   1005   \n",
       "599  Why APCON? At APCON, we are committed to setti...   1006   \n",
       "\n",
       "                                               CV_text  label  \n",
       "0    **John Doe**  \\n123 Tech Lane  \\nPleasanton, C...      1  \n",
       "1    **John Smith**  \\n[LinkedIn Profile] | [Github...      1  \n",
       "2    **John D. Anderson**  \\n123 Tech Lane  \\nSan F...      1  \n",
       "3    **John Anderson**  \\n123 Tech Avenue  \\nPleasa...      0  \n",
       "4    **John Smith**  \\n123 Tech Lane  \\nSan Francis...      0  \n",
       "..                                                 ...    ...  \n",
       "595  **John Smith**  \\n123 Tech Drive  \\nSan Jose, ...      1  \n",
       "596  **Resume**\\n\\n**John Doe**  \\n123 Main Street ...      1  \n",
       "597  **John Doe**  \\n[Your Address]  \\n[City, State...      0  \n",
       "598  **John Doe**  \\n1234 Elm Street  \\nCityville, ...      0  \n",
       "599  **Emma Johnson**  \\n123 Maple Street  \\nAustin...      0  \n",
       "\n",
       "[600 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the CSV file into a dataframe\n",
    "file_path = \"/Users/timg/Desktop/Workproject/my_fork/workproject_matching_algo/synthetic_data_csv/df_synth_data.csv\"\n",
    "df_synth_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the import\n",
    "df_synth_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58ae62-05ed-421f-900e-c58e7d8ae378",
   "metadata": {},
   "source": [
    "## 1.2 Importing Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e18af7d-5c26-48e5-9516-e77efa2eb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/workproject/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Define project root paths\n",
    "PROJECT_ROOT = '/Users/timg/Desktop/Workproject/my_fork/workproject_matching_algo'\n",
    "SERVICES_DIR = os.path.join(PROJECT_ROOT, 'services')\n",
    "RESOURCES_DIR = os.path.join(PROJECT_ROOT, 'Resources', 'data')\n",
    "\n",
    "# Add the paths to sys.path if not already present\n",
    "for module_path in [SERVICES_DIR, PROJECT_ROOT]:\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "# Import the necessary functions\n",
    "from main import calc_similarity, calc_similarity_sbs, job_info_extraction, resume_extraction  # Import the necessary functions\n",
    "\n",
    "# Define the path to your skills patterns file\n",
    "skills_patterns_path = os.path.join(RESOURCES_DIR, 'skills.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860c2d1-f57a-4519-b3e6-1e69b6669dc7",
   "metadata": {},
   "source": [
    "## 1.3 Setting up the Baseline Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393fb79-38a6-45c3-b4eb-c975e35da882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main_synth_df(synth_data):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    similarity_results = []\n",
    "\n",
    "    # Iterate over each unique job description ID in the dataset\n",
    "    for jd_id in synth_data['JD_ID'].unique():\n",
    "        # Filter the data for the current job description\n",
    "        job_data = synth_data[synth_data['JD_ID'] == jd_id]\n",
    "\n",
    "        # Extract the job description text\n",
    "        jd_text = job_data.iloc[0]['JD_text']\n",
    "        df_jobs = pd.DataFrame([jd_text], columns=[\"raw\"])\n",
    "        df_jobs = job_info_extraction(df_jobs)  # Use job_info_extraction to extract job description skills\n",
    "        \n",
    "        # Extract resumes for this job description\n",
    "        resumes = job_data[['CV_ID', 'CV_text']].copy()  # Include necessary columns\n",
    "        resumes['name'] = resumes['CV_ID']  # Add a \"name\" column derived from CV_ID for compatibility\n",
    "        resumes['raw'] = resumes['CV_text']  # Add a \"raw\" column for compatibility\n",
    "        df_resumes = resume_extraction(resumes)  # Use resume_extraction to extract resume skills\n",
    "        \n",
    "        # Calculate similarity for the job description and its related resumes\n",
    "        analysis_data_df = calc_similarity(df_resumes, df_jobs, parallel=True)\n",
    "        \n",
    "        # Add similarity scores back to the original data\n",
    "        job_data = job_data.reset_index(drop=True)\n",
    "        job_data['Similarity_score'] = analysis_data_df['similarity_score']  # Assuming calc_similarity outputs this\n",
    "\n",
    "        # Append to results\n",
    "        similarity_results.append(job_data)\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    df_final = pd.concat(similarity_results, ignore_index=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    print(f\"Processing Time: {dt*1000:.2f}ms\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7910a-fea9-492a-9d79-872de585f902",
   "metadata": {},
   "source": [
    "## 1.4 Running the Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887aa51-5016-4dd6-bbd7-45966feb41eb",
   "metadata": {},
   "source": [
    "### 1.4.1 First Attempt\n",
    "\n",
    "**Specs:**\n",
    "- 100 JDs a 6 CVs\n",
    "- \"almost perfect\" vs. \"slightly bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba764c54-c12f-480e-a2c1-520423e0bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = main_synth_df(df_synth_data)\n",
    "\n",
    "df_result.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209f261-ab63-4aa7-ba05-727bac6ede0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'label_predicted' where top 3 resumes = 1 and the rest = 0\n",
    "df_result = df_result.sort_values(['JD_ID', 'Similarity_score'], ascending=[True, False])\n",
    "\n",
    "# Apply the labeling logic within each JD_ID group\n",
    "df_result['label_predicted'] = df_result.groupby('JD_ID')['Similarity_score'].rank(ascending=False).apply(lambda x: 1 if x <= 3 else 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a132309-bac1-46ab-9682-56e8a65120ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[\"label_predicted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6d3c7-d725-4c15-9fd9-58bf72178029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(df_result['label'], df_result['label_predicted'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3e3dd-fda8-4361-8ee8-ee14278ee673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and median similarity scores for label = 1\n",
    "mean_label_1 = round(df_result[df_result['label'] == 1]['Similarity_score'].mean(), 4)\n",
    "median_label_1 = round(df_result[df_result['label'] == 1]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate mean and median similarity scores for label = 0\n",
    "mean_label_0 = round(df_result[df_result['label'] == 0]['Similarity_score'].mean(), 4)\n",
    "median_label_0 = round(df_result[df_result['label'] == 0]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate differences between label 1 and label 0\n",
    "mean_difference = round(mean_label_1 - mean_label_0, 4)\n",
    "median_difference = round(median_label_1 - median_label_0, 4)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Label 1 - Mean Similarity Score: {mean_label_1}, Median Similarity Score: {median_label_1}\")\n",
    "print(f\"Label 0 - Mean Similarity Score: {mean_label_0}, Median Similarity Score: {median_label_0}\")\n",
    "print(f\"Difference - Mean: {mean_difference}, Median: {median_difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b30af-1e0e-4fd0-a122-17f6ad91f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Separate similarity scores for label = 1 and label = 0\n",
    "scores_label_1 = df_result[df_result['label'] == 1]['Similarity_score']\n",
    "scores_label_0 = df_result[df_result['label'] == 0]['Similarity_score']\n",
    "\n",
    "# Create histogram data for both labels with more bins (50 bins)\n",
    "bins = np.linspace(0, 1, 50)  # Define bins from 0 to 1 for the similarity scores\n",
    "hist_label_1, _ = np.histogram(scores_label_1, bins=bins, density=True)\n",
    "hist_label_0, _ = np.histogram(scores_label_0, bins=bins, density=True)\n",
    "\n",
    "# Calculate the bin centers for the line plot\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Plot the line graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bin_centers, hist_label_1, label='Label 1')\n",
    "plt.plot(bin_centers, hist_label_0, label='Label 0')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Similarity Scores by Label (Detailed Line Graph)', fontsize=16)\n",
    "plt.xlabel('Similarity Score', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97513342-c1b4-4364-a68d-67bfc5d37f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where \"label\" and \"label_predicted\" do not match\n",
    "df_mismatched = df_result[df_result['label'] != df_result['label_predicted']]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_mismatched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535de3df-6bc5-48cb-8466-1c91415b586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify JD_IDs with mismatched rows\n",
    "mismatched_jd_ids = df_result.loc[df_result['label'] != df_result['label_predicted'], 'JD_ID'].unique()\n",
    "\n",
    "# Filter the DataFrame to include only rows with these JD_IDs\n",
    "df_mismatched_jd_ids = df_result[df_result['JD_ID'].isin(mismatched_jd_ids)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_mismatched_jd_ids.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088307e-1dc8-43c3-be2e-c4b36bda3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by JD_ID and Similarity_score\n",
    "df_mismatched_jd_ids = df_mismatched_jd_ids.sort_values(by=['JD_ID', 'Similarity_score'], ascending=[True, False])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_mismatched_jd_ids.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a13259-674b-412a-9254-ee0ca2e19084",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.4.2 Second Attempt\n",
    "\n",
    "**Specs:**\n",
    "- Same, second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3b7f7-ed60-4765-a288-60c5b930d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "df_result_2 = main_synth_df(df_synth_data)\n",
    "\n",
    "df_result_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ced5cf-cd36-4911-8a1f-8068e7ec5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'label_predicted' where top 3 resumes = 1 and the rest = 0\n",
    "df_result_2 = df_result_2.sort_values(['JD_ID', 'Similarity_score'], ascending=[True, False])\n",
    "\n",
    "# Apply the labeling logic within each JD_ID group\n",
    "df_result_2['label_predicted'] = df_result_2.groupby('JD_ID')['Similarity_score'].rank(ascending=False).apply(lambda x: 1 if x <= 3 else 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_result_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0234d-63d1-493b-a36c-2513178507d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_2[\"label_predicted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71377bf9-fdbf-4a10-9be7-0716d104d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(df_result_2['label'], df_result_2['label_predicted'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe59216-e0b3-441d-b4f7-62850378dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and median similarity scores for label = 1\n",
    "mean_label_1 = round(df_result[df_result['label'] == 1]['Similarity_score'].mean(), 4)\n",
    "median_label_1 = round(df_result[df_result['label'] == 1]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate mean and median similarity scores for label = 0\n",
    "mean_label_0 = round(df_result[df_result['label'] == 0]['Similarity_score'].mean(), 4)\n",
    "median_label_0 = round(df_result[df_result['label'] == 0]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate differences between label 1 and label 0\n",
    "mean_difference = round(mean_label_1 - mean_label_0, 4)\n",
    "median_difference = round(median_label_1 - median_label_0, 4)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Label 1 - Mean Similarity Score: {mean_label_1}, Median Similarity Score: {median_label_1}\")\n",
    "print(f\"Label 0 - Mean Similarity Score: {mean_label_0}, Median Similarity Score: {median_label_0}\")\n",
    "print(f\"Difference - Mean: {mean_difference}, Median: {median_difference}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5c02d-c7c7-4157-99f3-9e448477df9a",
   "metadata": {},
   "source": [
    "### 1.4.3 Third Attempt\n",
    "\n",
    "**Specs:**\n",
    "- 20 JDs a 6 CVs\n",
    "- \"very good fit\" vs. \"moderate fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b79d61-45c7-4b81-90c4-b66ce86bff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the CSV file into a dataframe\n",
    "file_path = \"/Users/timg/Desktop/Workproject/my_fork/workproject_matching_algo/synthetic_data_csv/df_synth_data_2_test.csv\"\n",
    "df_synth_data_2_test = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the import\n",
    "df_synth_data_2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c316d-f9f4-48d5-aa2a-f09583d26aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_2_test = main_synth_df(df_synth_data_2_test)\n",
    "\n",
    "df_result_2_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf7f99-0b96-403a-9fdd-a335b7a094ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'label_predicted' where top 3 resumes = 1 and the rest = 0\n",
    "df_result_2_test = df_result_2_test.sort_values(['JD_ID', 'Similarity_score'], ascending=[True, False])\n",
    "\n",
    "# Apply the labeling logic within each JD_ID group\n",
    "df_result_2_test['label_predicted'] = df_result_2_test.groupby('JD_ID')['Similarity_score'].rank(ascending=False).apply(lambda x: 1 if x <= 3 else 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_result_2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f118f1f-838d-4bc7-bd0b-efe28b015c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_2_test[\"label_predicted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f668b7b9-cc7d-4b4f-bca5-2bac41dcf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm_2_test = confusion_matrix(df_result_2_test['label'], df_result_2_test['label_predicted'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp_2_test = ConfusionMatrixDisplay(confusion_matrix=cm_2_test, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "disp_2_test.plot(cmap=\"Blues\", values_format=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd74f0-7318-4bbc-84e0-1010040af5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and median similarity scores for label = 1\n",
    "mean_label_1_2_test = round(df_result_2_test[df_result_2_test['label'] == 1]['Similarity_score'].mean(), 4)\n",
    "median_label_1_2_test = round(df_result_2_test[df_result_2_test['label'] == 1]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate mean and median similarity scores for label = 0\n",
    "mean_label_0_2_test = round(df_result_2_test[df_result_2_test['label'] == 0]['Similarity_score'].mean(), 4)\n",
    "median_label_0_2_test = round(df_result_2_test[df_result_2_test['label'] == 0]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate differences between label 1 and label 0\n",
    "mean_difference_2_test = round(mean_label_1_2_test - mean_label_0_2_test, 4)\n",
    "median_difference_2_test = round(median_label_1_2_test - median_label_0_2_test, 4)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Label 1 - Mean Similarity Score: {mean_label_1_2_test}, Median Similarity Score: {median_label_1_2_test}\")\n",
    "print(f\"Label 0 - Mean Similarity Score: {mean_label_0_2_test}, Median Similarity Score: {median_label_0_2_test}\")\n",
    "print(f\"Difference - Mean: {mean_difference_2_test}, Median: {median_difference_2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f8d38-9082-4b57-9e37-51e1a24256d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Separate similarity scores for label = 1 and label = 0\n",
    "scores_label_1 = df_result_2_test[df_result_2_test['label'] == 1]['Similarity_score']\n",
    "scores_label_0 = df_result_2_test[df_result_2_test['label'] == 0]['Similarity_score']\n",
    "\n",
    "# Create histogram data for both labels with more bins (50 bins)\n",
    "bins = np.linspace(0, 1, 50)  # Define bins from 0 to 1 for the similarity scores\n",
    "hist_label_1, _ = np.histogram(scores_label_1, bins=bins, density=True)\n",
    "hist_label_0, _ = np.histogram(scores_label_0, bins=bins, density=True)\n",
    "\n",
    "# Calculate the bin centers for the line plot\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Plot the line graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bin_centers, hist_label_1, label='Label 1')\n",
    "plt.plot(bin_centers, hist_label_0, label='Label 0')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Similarity Scores by Label (Detailed Line Graph)', fontsize=16)\n",
    "plt.xlabel('Similarity Score', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e77af-bb60-4d40-ba65-936f2ef9fd33",
   "metadata": {},
   "source": [
    "### 1.2.4 Fourth Attempt\n",
    "\n",
    "**Specs:**\n",
    "- 20 JDs a 6 CVs\n",
    "- \"almost perfect fit\" vs. \"slightly bad to moderate fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2502f4-5cd7-46c5-9117-d2149e56b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the CSV file into a dataframe\n",
    "file_path = \"/Users/timg/Desktop/Workproject/my_fork/workproject_matching_algo/synthetic_data_csv/df_synth_data_3_test.csv\"\n",
    "df_synth_data_3_test = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the import\n",
    "df_synth_data_3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5dbec3-645a-45d4-90d3-96aa33180897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_3_test = main_synth_df(df_synth_data_3_test)\n",
    "\n",
    "df_result_3_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fbde20-8d38-48c6-8afc-b09a12c65ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'label_predicted' where top 3 resumes = 1 and the rest = 0\n",
    "df_result_3_test = df_result_3_test.sort_values(['JD_ID', 'Similarity_score'], ascending=[True, False])\n",
    "\n",
    "# Apply the labeling logic within each JD_ID group\n",
    "df_result_3_test['label_predicted'] = df_result_3_test.groupby('JD_ID')['Similarity_score'].rank(ascending=False).apply(lambda x: 1 if x <= 3 else 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_result_3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754a67d-8601-4a66-8859-a418bc4b2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_3_test[\"label_predicted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c1e17-885f-4b0f-8a72-e12445995a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm_3_test = confusion_matrix(df_result_3_test['label'], df_result_3_test['label_predicted'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp_3_test = ConfusionMatrixDisplay(confusion_matrix=cm_3_test, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "disp_3_test.plot(cmap=\"Blues\", values_format=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28047e2b-4540-40c6-a407-e15024d4db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and median similarity scores for label = 1\n",
    "mean_label_1_3_test = round(df_result_3_test[df_result_3_test['label'] == 1]['Similarity_score'].mean(), 4)\n",
    "median_label_1_3_test = round(df_result_3_test[df_result_3_test['label'] == 1]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate mean and median similarity scores for label = 0\n",
    "mean_label_0_3_test = round(df_result_3_test[df_result_3_test['label'] == 0]['Similarity_score'].mean(), 4)\n",
    "median_label_0_3_test = round(df_result_3_test[df_result_3_test['label'] == 0]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate differences between label 1 and label 0\n",
    "mean_difference_3_test = round(mean_label_1_3_test - mean_label_0_3_test, 4)\n",
    "median_difference_3_test = round(median_label_1_3_test - median_label_0_3_test, 4)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Label 1 - Mean Similarity Score: {mean_label_1_3_test}, Median Similarity Score: {median_label_1_3_test}\")\n",
    "print(f\"Label 0 - Mean Similarity Score: {mean_label_0_3_test}, Median Similarity Score: {median_label_0_3_test}\")\n",
    "print(f\"Difference - Mean: {mean_difference_3_test}, Median: {median_difference_3_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee021f9-59d9-4ff6-809d-4bdb44804c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Separate similarity scores for label = 1 and label = 0\n",
    "scores_label_1 = df_result_3_test[df_result_3_test['label'] == 1]['Similarity_score']\n",
    "scores_label_0 = df_result_3_test[df_result_3_test['label'] == 0]['Similarity_score']\n",
    "\n",
    "# Create histogram data for both labels with more bins (50 bins)\n",
    "bins = np.linspace(0, 1, 50)  # Define bins from 0 to 1 for the similarity scores\n",
    "hist_label_1, _ = np.histogram(scores_label_1, bins=bins, density=True)\n",
    "hist_label_0, _ = np.histogram(scores_label_0, bins=bins, density=True)\n",
    "\n",
    "# Calculate the bin centers for the line plot\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Plot the line graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bin_centers, hist_label_1, label='Label 1')\n",
    "plt.plot(bin_centers, hist_label_0, label='Label 0')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Similarity Scores by Label (Detailed Line Graph)', fontsize=16)\n",
    "plt.xlabel('Similarity Score', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5d85e-f998-4089-a049-4944cb5ed4ad",
   "metadata": {},
   "source": [
    "### 1.4.5 Fifth Attempt\n",
    "\n",
    "**Specs:**\n",
    "- **100 JDs** a 6 CVs\n",
    "- \"almost perfect fit\" vs. \"slightly bad to moderate fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a848b-8224-4954-b20c-57b5ea11c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import the CSV file into a dataframe\n",
    "file_path = \"/Users/timg/Desktop/Workproject/my_fork/workproject_matching_algo/synthetic_data_csv/df_synth_data_4_100.csv\"\n",
    "df_synth_data_4_100 = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the import\n",
    "df_synth_data_4_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914fe075-162c-4894-b168-e5037a250553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_4_100 = main_synth_df(df_synth_data_4_100)\n",
    "\n",
    "df_result_4_100.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a269a78-d47d-46c9-8ce2-1ce036f499c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'label_predicted' where top 3 resumes = 1 and the rest = 0\n",
    "df_result_4_100 = df_result_4_100.sort_values(['JD_ID', 'Similarity_score'], ascending=[True, False])\n",
    "\n",
    "# Apply the labeling logic within each JD_ID group\n",
    "df_result_4_100['label_predicted'] = df_result_4_100.groupby('JD_ID')['Similarity_score'].rank(ascending=False).apply(lambda x: 1 if x <= 3 else 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_result_4_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d2d2b-8370-4f61-83b3-b9896866a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_4_100[\"label_predicted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda72fe0-11ca-4a37-a658-71eb22f9e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm_4_100 = confusion_matrix(df_result_4_100['label'], df_result_4_100['label_predicted'])\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp_4_100 = ConfusionMatrixDisplay(confusion_matrix=cm_4_100, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "disp_4_100.plot(cmap=\"Blues\", values_format=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739575c-545f-4dab-8197-41ab354e3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and median similarity scores for label = 1\n",
    "mean_label_1_4_100 = round(df_result_4_100[df_result_4_100['label'] == 1]['Similarity_score'].mean(), 4)\n",
    "median_label_1_4_100 = round(df_result_4_100[df_result_4_100['label'] == 1]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate mean and median similarity scores for label = 0\n",
    "mean_label_0_4_100 = round(df_result_4_100[df_result_4_100['label'] == 0]['Similarity_score'].mean(), 4)\n",
    "median_label_0_4_100 = round(df_result_4_100[df_result_4_100['label'] == 0]['Similarity_score'].median(), 4)\n",
    "\n",
    "# Calculate differences between label 1 and label 0\n",
    "mean_difference_4_100 = round(mean_label_1_4_100 - mean_label_0_4_100, 4)\n",
    "median_difference_4_100 = round(median_label_1_4_100 - median_label_0_4_100, 4)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Label 1 - Mean Similarity Score: {mean_label_1_4_100}, Median Similarity Score: {median_label_1_4_100}\")\n",
    "print(f\"Label 0 - Mean Similarity Score: {mean_label_0_4_100}, Median Similarity Score: {median_label_0_4_100}\")\n",
    "print(f\"Difference - Mean: {mean_difference_4_100}, Median: {median_difference_4_100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247f247-38a5-49b6-8f61-3f7b9ce5e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Separate similarity scores for label = 1 and label = 0\n",
    "scores_label_1 = df_result_4_100[df_result_4_100['label'] == 1]['Similarity_score']\n",
    "scores_label_0 = df_result_4_100[df_result_4_100['label'] == 0]['Similarity_score']\n",
    "\n",
    "# Create histogram data for both labels with more bins (50 bins)\n",
    "bins = np.linspace(0, 1, 50)  # Define bins from 0 to 1 for the similarity scores\n",
    "hist_label_1, _ = np.histogram(scores_label_1, bins=bins, density=True)\n",
    "hist_label_0, _ = np.histogram(scores_label_0, bins=bins, density=True)\n",
    "\n",
    "# Calculate the bin centers for the line plot\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Plot the line graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bin_centers, hist_label_1, label='Label 1')\n",
    "plt.plot(bin_centers, hist_label_0, label='Label 0')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Similarity Scores by Label (Detailed Line Graph)', fontsize=16)\n",
    "plt.xlabel('Similarity Score', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5dbc1-e1d0-4d86-9a58-fd50d3579ea7",
   "metadata": {},
   "source": [
    "# 2 Improving the Model Components\n",
    "\n",
    "## Model Pipeline\n",
    "1. **Skills Extraction:** Skills Dictionary (baseline) vs. Taxonomy\n",
    "2. **Input for Embeddings:** Bunch-of-skills (baseline) vs. Skill-by-skill\n",
    "3. **Embedding Models:** all-mpnet-base-v2 (baseline) vs. Huggingface leaderboard Models\n",
    "4. **Modelling Approach:** Bi-Encoder (baseline) vs. Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672067dc-8847-4f4f-b489-8e93f8362821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Analysis Function for 1 df\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "def performance_analysis(df):\n",
    "    # Dynamically get the name of the DataFrame passed to the function\n",
    "    df_name = [key for key, value in globals().items() if value is df][0]\n",
    "\n",
    "    # 1. Count \"label_predicted\" values\n",
    "    print(f\"\\n{df_name} - Label Predicted Counts:\")\n",
    "    print(df[\"label_predicted\"].value_counts())\n",
    "\n",
    "    # 2. Mean and median similarity scores\n",
    "    mean_label_1 = round(df[df['label'] == 1]['Similarity_score'].mean(), 4)\n",
    "    median_label_1 = round(df[df['label'] == 1]['Similarity_score'].median(), 4)\n",
    "    mean_label_0 = round(df[df['label'] == 0]['Similarity_score'].mean(), 4)\n",
    "    median_label_0 = round(df[df['label'] == 0]['Similarity_score'].median(), 4)\n",
    "    mean_difference = round(mean_label_1 - mean_label_0, 4)\n",
    "    median_difference = round(median_label_1 - median_label_0, 4)\n",
    "\n",
    "    print(f\"\\n{df_name} - Similarity Scores:\")\n",
    "    print(f\"Label 1 - Mean: {mean_label_1}, Median: {median_label_1}\")\n",
    "    print(f\"Label 0 - Mean: {mean_label_0}, Median: {median_label_0}\")\n",
    "    print(f\"Difference - Mean: {mean_difference}, Median: {median_difference}\")\n",
    "\n",
    "    # 3. Distribution plot for similarity scores\n",
    "    scores_label_1 = df[df['label'] == 1]['Similarity_score']\n",
    "    scores_label_0 = df[df['label'] == 0]['Similarity_score']\n",
    "\n",
    "    bins = np.linspace(0, 1, 50)\n",
    "    hist_label_1, _ = np.histogram(scores_label_1, bins=bins, density=True)\n",
    "    hist_label_0, _ = np.histogram(scores_label_0, bins=bins, density=True)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(bin_centers, hist_label_1, label=\"Label 1\")\n",
    "    plt.plot(bin_centers, hist_label_0, label=\"Label 0\")\n",
    "    plt.title(f'Distribution of Similarity Scores by Label ({df_name})', fontsize=16)\n",
    "    plt.xlabel('Similarity Score', fontsize=14)\n",
    "    plt.ylabel('Density', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Confusion matrix\n",
    "    cm = confusion_matrix(df['label'], df['label_predicted'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(f\"Confusion Matrix ({df_name})\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # 5. Accuracy, Precision, Recall, and F1-Score\n",
    "    accuracy = accuracy_score(df['label'], df['label_predicted'])\n",
    "    precision = precision_score(df['label'], df['label_predicted'])\n",
    "    recall = recall_score(df['label'], df['label_predicted'])\n",
    "    f1 = f1_score(df['label'], df['label_predicted'])\n",
    "\n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            df_name: [accuracy, precision, recall, f1],\n",
    "        },\n",
    "        index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "    )\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(metrics_df)\n",
    "\n",
    "    # 6. Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(df['label'], df['Similarity_score'])\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(recall, precision, label=df_name)\n",
    "    plt.title(\"Precision-Recall Curve\", fontsize=16)\n",
    "    plt.xlabel(\"Recall\", fontsize=14)\n",
    "    plt.ylabel(\"Precision\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff74437-1a53-41ed-825c-219975aca917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Analysis Function for 2 dfs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "def performance_comparison(df1, df2, df1_name=\"DF1\", df2_name=\"DF2\"):\n",
    "    # 1. Count \"label_predicted\" values\n",
    "    print(f\"\\n{df1_name} - Label Predicted Counts:\")\n",
    "    print(df1[\"label_predicted\"].value_counts())\n",
    "    print(f\"\\n{df2_name} - Label Predicted Counts:\")\n",
    "    print(df2[\"label_predicted\"].value_counts())\n",
    "\n",
    "    # 2. Mean and median similarity scores\n",
    "    def calculate_similarity_stats(df, name):\n",
    "        mean_label_1 = round(df[df['label'] == 1]['Similarity_score'].mean(), 4)\n",
    "        median_label_1 = round(df[df['label'] == 1]['Similarity_score'].median(), 4)\n",
    "        mean_label_0 = round(df[df['label'] == 0]['Similarity_score'].mean(), 4)\n",
    "        median_label_0 = round(df[df['label'] == 0]['Similarity_score'].median(), 4)\n",
    "        mean_difference = round(mean_label_1 - mean_label_0, 4)\n",
    "        median_difference = round(median_label_1 - median_label_0, 4)\n",
    "\n",
    "        print(f\"\\n{name} - Similarity Scores:\")\n",
    "        print(f\"Label 1 - Mean: {mean_label_1}, Median: {median_label_1}\")\n",
    "        print(f\"Label 0 - Mean: {mean_label_0}, Median: {median_label_0}\")\n",
    "        print(f\"Difference - Mean: {mean_difference}, Median: {median_difference}\")\n",
    "\n",
    "    calculate_similarity_stats(df1, df1_name)\n",
    "    calculate_similarity_stats(df2, df2_name)\n",
    "\n",
    "    # 3. Distribution plot for similarity scores\n",
    "    def plot_similarity_distribution(df, name):\n",
    "        scores_label_1 = df[df['label'] == 1]['Similarity_score']\n",
    "        scores_label_0 = df[df['label'] == 0]['Similarity_score']\n",
    "\n",
    "        bins = np.linspace(0, 1, 50)\n",
    "        hist_label_1, _ = np.histogram(scores_label_1, bins=bins, density=True)\n",
    "        hist_label_0, _ = np.histogram(scores_label_0, bins=bins, density=True)\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(bin_centers, hist_label_1, label=\"Label 1\")\n",
    "        plt.plot(bin_centers, hist_label_0, label=\"Label 0\")\n",
    "        plt.title(f'Distribution of Similarity Scores by Label ({name})', fontsize=16)\n",
    "        plt.xlabel('Similarity Score', fontsize=14)\n",
    "        plt.ylabel('Density', fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.show()\n",
    "\n",
    "    plot_similarity_distribution(df1, df1_name)\n",
    "    plot_similarity_distribution(df2, df2_name)\n",
    "\n",
    "    # 4. Confusion matrix\n",
    "    def plot_confusion_matrix(df, name):\n",
    "        cm = confusion_matrix(df['label'], df['label_predicted'])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\"])\n",
    "        disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "        plt.title(f\"Confusion Matrix ({name})\", fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    plot_confusion_matrix(df1, df1_name)\n",
    "    plot_confusion_matrix(df2, df2_name)\n",
    "\n",
    "    # 5. Accuracy, Precision, Recall, and F1-Score\n",
    "    def calculate_metrics(df):\n",
    "        accuracy = accuracy_score(df['label'], df['label_predicted'])\n",
    "        precision = precision_score(df['label'], df['label_predicted'])\n",
    "        recall = recall_score(df['label'], df['label_predicted'])\n",
    "        f1 = f1_score(df['label'], df['label_predicted'])\n",
    "        return [accuracy, precision, recall, f1]\n",
    "\n",
    "    metrics_df = pd.DataFrame(\n",
    "        {\n",
    "            df1_name: calculate_metrics(df1),\n",
    "            df2_name: calculate_metrics(df2),\n",
    "        },\n",
    "        index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "    )\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(metrics_df)\n",
    "\n",
    "    # 6. Precision-Recall Curve\n",
    "    def plot_precision_recall(df, name):\n",
    "        precision, recall, _ = precision_recall_curve(df['label'], df['Similarity_score'])\n",
    "        plt.plot(recall, precision, label=name)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_precision_recall(df1, df1_name)\n",
    "    plot_precision_recall(df2, df2_name)\n",
    "    plt.title(\"Precision-Recall Curve\", fontsize=16)\n",
    "    plt.xlabel(\"Recall\", fontsize=14)\n",
    "    plt.ylabel(\"Precision\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b22a6-b9de-45aa-99ff-71f79b9c6ba3",
   "metadata": {},
   "source": [
    "## 2.1 Skills Extraction: Skills Dictionary (baseline) vs. Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67d73a-4336-4879-8fce-35b9e0baed28",
   "metadata": {},
   "source": [
    "## 2.2 Input for Embeddings: Bunch-of-skills (baseline) vs. Skill-by-skill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22056bf9-a831-43bb-81f8-4f4dd93e3998",
   "metadata": {},
   "source": [
    "### 2.2.1 Bag-of-skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8164a-a0a2-451d-84d0-9fc5b3f2a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_analysis(df_result_4_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1f427-b733-400e-9750-a26444144b2b",
   "metadata": {},
   "source": [
    "### 2.2.2 Skill-by-skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936c442-d1a2-413d-8379-a74a16109c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a645c2b-22d0-4c30-9eaa-208061d7fe05",
   "metadata": {},
   "source": [
    "## 2.3 Embedding Models: all-mpnet-base-v2 (baseline) vs. Huggingface leaderboard Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e10830d-cab6-496e-b3b1-ceb7b613f7bb",
   "metadata": {},
   "source": [
    "### 2.3.1 all-mpnet-base-v2\n",
    "\n",
    "**Model Characteristics:**\n",
    "- Model size: \n",
    "- Embedding dimensions: \n",
    "- Tokenizer:\n",
    "- Memory Usage (GB, fp32): \n",
    "- Max Tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ab5c5-980c-4cb1-b7ad-03ff3193d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In main.py and imported into the Notebook\n",
    "def calc_similarity(applicant_df, job_df, N=3, parallel=False):\n",
    "    \"\"\"Calculate cosine similarity based on MPNET embeddings of combined skills.\"\"\"\n",
    "\n",
    "    # Initialize the model once outside the loop for efficiency\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    model.max_seq_length = 75\n",
    "    model.tokenizer.padding_side=\"right\"\n",
    "    model.eval()\n",
    "\n",
    "    def add_eos(input_examples):\n",
    "        \"\"\" helper function to add special tokens between each skills\"\"\"\n",
    "        input_examples = [input_example + model.tokenizer.eos_token for input_example in input_examples]\n",
    "        return input_examples\n",
    "\n",
    "    # Precompute job embeddings\n",
    "    job_df['Skills_Text'] = job_df['Skills'].apply(add_eos)\n",
    "    job_df['Skills_Text'] = job_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    job_embeddings = model.encode(\n",
    "    job_df['Skills_Text'].tolist())\n",
    "    # Precompute applicant embeddings\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills'].apply(add_eos)\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    applicant_embeddings = model.encode(\n",
    "    applicant_df['Skills_Text'].tolist(),\n",
    "        batch_size=32,\n",
    "        num_workers=os.cpu_count() // 2 if parallel else 0,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(job_embeddings, applicant_embeddings)\n",
    "\n",
    "    # Create a DataFrame from the similarity matrix\n",
    "    similarity_df = pd.DataFrame(similarity_matrix.T, index=applicant_df['name'], columns=job_df.index)\n",
    "    similarity_df = similarity_df.reset_index().melt(id_vars='name', var_name='job_id', value_name='similarity_score')\n",
    "    similarity_df['rank'] = similarity_df.groupby('job_id')['similarity_score'].rank(ascending=False)\n",
    "    similarity_df['interview_status'] = similarity_df['rank'].apply(lambda x: 'Selected' if x <= N else 'Not Selected')\n",
    "\n",
    "    return similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77df4c4-c865-4cdf-9b06-25e26ba58221",
   "metadata": {},
   "source": [
    "### 2.3.2 all-MiniLM-L6-v2\n",
    "\n",
    "**Model Characteristics:**\n",
    "- Model size: 22.7M params\n",
    "- Embedding dimensions: \n",
    "- Tokenizer:\n",
    "- Memory Usage (GB, fp32): \n",
    "- Max Tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b37b3-90a6-49cc-891e-1d6e98bc8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In main.py and imported into the Notebook\n",
    "def calc_similarity(applicant_df, job_df, N=3, parallel=False):\n",
    "    \"\"\"Calculate cosine similarity based on MPNET embeddings of combined skills.\"\"\"\n",
    "\n",
    "    # Initialize the model once outside the loop for efficiency\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    model.max_seq_length = 75\n",
    "    model.tokenizer.padding_side=\"right\"\n",
    "    model.eval()\n",
    "\n",
    "    def add_eos(input_examples):\n",
    "        \"\"\" helper function to add special tokens between each skills\"\"\"\n",
    "        input_examples = [input_example + model.tokenizer.eos_token for input_example in input_examples]\n",
    "        return input_examples\n",
    "\n",
    "    # Precompute job embeddings\n",
    "    job_df['Skills_Text'] = job_df['Skills'].apply(add_eos)\n",
    "    job_df['Skills_Text'] = job_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    job_embeddings = model.encode(\n",
    "    job_df['Skills_Text'].tolist())\n",
    "    # Precompute applicant embeddings\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills'].apply(add_eos)\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    applicant_embeddings = model.encode(\n",
    "    applicant_df['Skills_Text'].tolist(),\n",
    "        batch_size=32,\n",
    "        num_workers=os.cpu_count() // 2 if parallel else 0,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(job_embeddings, applicant_embeddings)\n",
    "\n",
    "    # Create a DataFrame from the similarity matrix\n",
    "    similarity_df = pd.DataFrame(similarity_matrix.T, index=applicant_df['name'], columns=job_df.index)\n",
    "    similarity_df = similarity_df.reset_index().melt(id_vars='name', var_name='job_id', value_name='similarity_score')\n",
    "    similarity_df['rank'] = similarity_df.groupby('job_id')['similarity_score'].rank(ascending=False)\n",
    "    similarity_df['interview_status'] = similarity_df['rank'].apply(lambda x: 'Selected' if x <= N else 'Not Selected')\n",
    "\n",
    "    return similarity_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2d307e7-301e-4ac5-926d-77d1c7be56bc",
   "metadata": {},
   "source": [
    "### 2.3.2 voyage-large-2-instruct\n",
    "\n",
    "**Model Characteristics:**\n",
    "- Model size: \n",
    "- Embedding dimensions: \n",
    "- Tokenizer:\n",
    "- Memory Usage (GB, fp32): \n",
    "- Max Tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17759bc1-bc77-41c2-98ea-5f89d337fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# In main.py and imported into the Notebook\n",
    "def calc_similarity(applicant_df, job_df, N=3, parallel=False):\n",
    "    \"\"\"Calculate cosine similarity based on NV-Embed-v2 embeddings of combined skills.\"\"\"\n",
    "\n",
    "    '''# Initialize the model once outside the loop for efficiency\n",
    "    model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
    "    model.max_seq_length = 75\n",
    "    model.tokenizer.padding_side=\"right\"'''\n",
    "\n",
    "    def add_eos(input_examples):\n",
    "        \"\"\" helper function to add special tokens between each skills\"\"\"\n",
    "        input_examples = [input_example + model.tokenizer.eos_token for input_example in input_examples]\n",
    "        return input_examples\n",
    "\n",
    "    # Precompute job embeddings\n",
    "    job_df['Skills_Text'] = job_df['Skills'].apply(add_eos)\n",
    "    job_df['Skills_Text'] = job_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    job_embeddings = model.encode(\n",
    "    job_df['Skills_Text'].tolist())\n",
    "    # Precompute applicant embeddings\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills'].apply(add_eos)\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    applicant_embeddings = model.encode(\n",
    "    applicant_df['Skills_Text'].tolist(),\n",
    "        batch_size=32,\n",
    "        num_workers=os.cpu_count() // 2 if parallel else 0,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(job_embeddings, applicant_embeddings)\n",
    "\n",
    "    # Create a DataFrame from the similarity matrix\n",
    "    similarity_df = pd.DataFrame(similarity_matrix.T, index=applicant_df['name'], columns=job_df.index)\n",
    "    similarity_df = similarity_df.reset_index().melt(id_vars='name', var_name='job_id', value_name='similarity_score')\n",
    "    similarity_df['rank'] = similarity_df.groupby('job_id')['similarity_score'].rank(ascending=False)\n",
    "    similarity_df['interview_status'] = similarity_df['rank'].apply(lambda x: 'Selected' if x <= N else 'Not Selected')\n",
    "\n",
    "    return similarity_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7791346a-20ae-4469-ba4d-ccc7c95cb056",
   "metadata": {},
   "source": [
    "### 2.3.3 BinGSE-Meta-Llama-3-8B-Instruct\n",
    "\n",
    "**Model Characteristics:**\n",
    "- Model size: \n",
    "- Embedding dimensions: \n",
    "- Tokenizer:\n",
    "- Memory Usage (GB, fp32): \n",
    "- Max Tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec467b0-dc13-4f8e-93bb-43ecd9a2046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# In main.py and imported into the Notebook\n",
    "def calc_similarity(applicant_df, job_df, N=3, parallel=False):\n",
    "    \"\"\"Calculate cosine similarity based on NV-Embed-v2 embeddings of combined skills.\"\"\"\n",
    "\n",
    "    # Loading base Meta-Llama-3 model, along with custom code that enables bidirectional connections in decoder-only LLMs.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
    "    )\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\", trust_remote_code=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "        trust_remote_code=True,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    # Loading MNTP (Masked Next Token Prediction) model.\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "    )\n",
    "\n",
    "    model = model.merge_and_unload()  # This can take several minutes on cpu\n",
    "\n",
    "    # Loading BinGSE model. This loads the trained LoRA weights on top of MNTP model. Hence the final weights are -- Base model + MNTP (LoRA) + BinGSE (LoRA).\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model, model_path \n",
    "    )\n",
    "\n",
    "    def add_eos(input_examples):\n",
    "        \"\"\" helper function to add special tokens between each skills\"\"\"\n",
    "        input_examples = [input_example + model.tokenizer.eos_token for input_example in input_examples]\n",
    "        return input_examples\n",
    "\n",
    "    # Precompute job embeddings\n",
    "    job_df['Skills_Text'] = job_df['Skills'].apply(add_eos)\n",
    "    job_df['Skills_Text'] = job_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    job_embeddings = model.encode(\n",
    "    job_df['Skills_Text'].tolist())\n",
    "    # Precompute applicant embeddings\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills'].apply(add_eos)\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    applicant_embeddings = model.encode(\n",
    "    applicant_df['Skills_Text'].tolist(),\n",
    "        batch_size=32,\n",
    "        num_workers=os.cpu_count() // 2 if parallel else 0,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(job_embeddings, applicant_embeddings)\n",
    "\n",
    "    # Create a DataFrame from the similarity matrix\n",
    "    similarity_df = pd.DataFrame(similarity_matrix.T, index=applicant_df['name'], columns=job_df.index)\n",
    "    similarity_df = similarity_df.reset_index().melt(id_vars='name', var_name='job_id', value_name='similarity_score')\n",
    "    similarity_df['rank'] = similarity_df.groupby('job_id')['similarity_score'].rank(ascending=False)\n",
    "    similarity_df['interview_status'] = similarity_df['rank'].apply(lambda x: 'Selected' if x <= N else 'Not Selected')\n",
    "\n",
    "    return similarity_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b59316c-d6fe-4814-9538-ebd28a36c7a6",
   "metadata": {},
   "source": [
    "### 2.3.4 NV-Embed-v2\n",
    "\n",
    "**Model Characteristics:**\n",
    "- Model size: \n",
    "- Embedding dimensions: \n",
    "- Tokenizer:\n",
    "- Memory Usage (GB, fp32): \n",
    "- Max Tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956b1a8-9774-426c-b36f-0204aa435cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# In main.py and imported into the Notebook\n",
    "def calc_similarity(applicant_df, job_df, N=3, parallel=False):\n",
    "    \"\"\"Calculate cosine similarity based on NV-Embed-v2 embeddings of combined skills.\"\"\"\n",
    "\n",
    "    # Initialize the model once outside the loop for efficiency\n",
    "    model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
    "    model.max_seq_length = 75\n",
    "    model.tokenizer.padding_side=\"right\"\n",
    "\n",
    "    def add_eos(input_examples):\n",
    "        \"\"\" helper function to add special tokens between each skills\"\"\"\n",
    "        input_examples = [input_example + model.tokenizer.eos_token for input_example in input_examples]\n",
    "        return input_examples\n",
    "\n",
    "    # Precompute job embeddings\n",
    "    job_df['Skills_Text'] = job_df['Skills'].apply(add_eos)\n",
    "    job_df['Skills_Text'] = job_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    job_embeddings = model.encode(\n",
    "    job_df['Skills_Text'].tolist())\n",
    "    # Precompute applicant embeddings\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills'].apply(add_eos)\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills_Text'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    applicant_embeddings = model.encode(\n",
    "    applicant_df['Skills_Text'].tolist(),\n",
    "        batch_size=32,\n",
    "        num_workers=os.cpu_count() // 2 if parallel else 0,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(job_embeddings, applicant_embeddings)\n",
    "\n",
    "    # Create a DataFrame from the similarity matrix\n",
    "    similarity_df = pd.DataFrame(similarity_matrix.T, index=applicant_df['name'], columns=job_df.index)\n",
    "    similarity_df = similarity_df.reset_index().melt(id_vars='name', var_name='job_id', value_name='similarity_score')\n",
    "    similarity_df['rank'] = similarity_df.groupby('job_id')['similarity_score'].rank(ascending=False)\n",
    "    similarity_df['interview_status'] = similarity_df['rank'].apply(lambda x: 'Selected' if x <= N else 'Not Selected')\n",
    "\n",
    "    return similarity_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5be0acc4-6b3a-4dbe-b56a-674b6692f9b9",
   "metadata": {},
   "source": [
    "### 2.3.5 SFR-Embedding-Mistral\n",
    "\n",
    "**Model Characteristics:**\n",
    "- Model size: \n",
    "- Embedding dimensions: \n",
    "- Tokenizer:\n",
    "- Memory Usage (GB, fp32): \n",
    "- Max Tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60997d51-5347-4adb-b5e3-11bcceedbd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32316f49-e255-4844-8a73-d8a1fac749d9",
   "metadata": {},
   "source": [
    "## 2.4 Modelling Approach: Bi-Encoder (baseline) vs. Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73764137-3d42-4e15-940d-d8e6cfe6911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victors Code for Cross-Encoder\n",
    "\n",
    "# Necessary imports\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Similarity Calculation using Cross-Encoder\n",
    "def calc_cross(applicant_df, job_df, N=3, parallel=False):\n",
    "    \"\"\" Use Cross Encoder to calculate similarity of combined skills.\"\"\"\n",
    "\n",
    "    # Initialize the model once outside the loop for efficiency\n",
    "    model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "     # Precompute job embeddings\n",
    "    job_df['Skills_Text'] = job_df['Skills'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    query = job_df['Skills_Text'][0]\n",
    "    # Precompute applicant embeddings\n",
    "    applicant_df['Skills_Text'] = applicant_df['Skills'].apply(lambda x: ' '.join(sorted(set(x))) if isinstance(x, list) else '')\n",
    "    applicants = applicant_df['Skills_Text'].tolist()\n",
    "\n",
    "    ranks = model.rank(\n",
    "        query,\n",
    "        applicants,\n",
    "        batch_size=32,\n",
    "        num_workers=os.cpu_count() // 2 if parallel else 0,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "\n",
    "    similarity_df = pd.DataFrame(ranks)\n",
    "    similarity_df['softmaxed'] = F.softmax(torch.tensor(similarity_df['score']))\n",
    "    similarity_df = similarity_df.join(applicant_df[[\"name\"]], on=\"corpus_id\")\n",
    "    \n",
    "    # similarity_df['interview_status'] = similarity_df.index.apply(lambda x: 'Selected' if x <= N else 'Not Selected')\n",
    "\n",
    "    return similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848f99c-852a-4a38-8260-95515945544b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workproject",
   "language": "python",
   "name": "workproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
