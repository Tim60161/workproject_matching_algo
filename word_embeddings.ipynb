{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\envs\\workproject\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model once outside the loop for efficiency\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_embeddings = model.encode(\"python\")\n",
    "\n",
    "cand_emded = model.encode(\"rust\")\n",
    "\n",
    "joker = model.encode(\"crocodile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2863149]]\n",
      "[[0.40933645]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(job_embeddings.reshape(1,-1), cand_emded.reshape(1,-1)))\n",
    "print(cosine_similarity(job_embeddings.reshape(1,-1), joker.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_embeddings = model.encode(\"python,c++,julia,oracle\")\n",
    "\n",
    "cand_emded = model.encode(\"rust,go,sql,php\")\n",
    "\n",
    "joker = model.encode(\"crocodile,lizard,boa,rattlesnake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34695765]]\n",
      "[[0.11074138]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(job_embeddings.reshape(1,-1), cand_emded.reshape(1,-1)))\n",
    "print(cosine_similarity(job_embeddings.reshape(1,-1), joker.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'Python':\n",
      "learning: 0.2707\n",
      "can: 0.2106\n",
      "processing: 0.1672\n",
      "languages: 0.1503\n",
      "popular: 0.1321\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Example corpus for training\n",
    "corpus = [\n",
    "    [\"Python\", \"is\", \"a\", \"popular\", \"programming\", \"language\"],\n",
    "    [\"Java\", \"and\", \"C++\", \"are\", \"also\", \"programming\", \"languages\"],\n",
    "    [\"Python\", \"can\", \"be\", \"used\", \"for\", \"machine\", \"learning\"],\n",
    "    [\"Gensim\", \"is\", \"a\", \"library\", \"for\", \"natural\", \"language\", \"processing\"],\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Find most similar words to \"Python\"\n",
    "similar_words = model.wv.most_similar(\"Python\", topn=5)\n",
    "\n",
    "# Display the similar words\n",
    "print(\"Most similar words to 'Python':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Word 'sus' is in the model.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download the Google News vectors\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pythons: 0.6688377857208252\n",
      "Burmese_python: 0.6680364608764648\n",
      "snake: 0.6606292724609375\n",
      "crocodile: 0.6591362953186035\n",
      "boa_constrictor: 0.6443519592285156\n",
      "alligator: 0.6421656608581543\n",
      "reptile: 0.6387744545936584\n",
      "albino_python: 0.6158879995346069\n",
      "croc: 0.6083583831787109\n",
      "lizard: 0.6013416647911072\n"
     ]
    }
   ],
   "source": [
    "if 'python' in model:\n",
    "    similar_words = model.most_similar('python', topn=10)\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"{word}: {similarity}\")\n",
    "else:\n",
    "    print(\"Word 'python' is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.most_similar('BERT', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similar_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, similarity \u001b[38;5;129;01min\u001b[39;00m similar_words:\n\u001b[0;32m      2\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'similar_words' is not defined"
     ]
    }
   ],
   "source": [
    "for word, similarity in similar_words:\n",
    "        print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/bash: line 2: wget: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B/glove.6B.50d.txt  \n",
      "  inflating: glove.6B/glove.6B.100d.txt  \n",
      "  inflating: glove.6B/glove.6B.200d.txt  \n",
      "  inflating: glove.6B/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Download the GloVe embeddings\n",
    "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "# Extract the GloVe embeddings\n",
    "unzip glove.6B.zip -d glove.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Convert GloVe format to Gensim format\n",
    "def load_glove_model(glove_file):\n",
    "    glove_vectors = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "    return glove_vectors\n",
    "\n",
    "# Load the 300-dimensional GloVe vectors\n",
    "glove_file = os.path.join(\"glove.6B\", \"glove.6B.300d.txt\")\n",
    "glove_model = load_glove_model(glove_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'python':\n",
      "monty: 0.6837380528450012\n",
      "perl: 0.519283652305603\n",
      "cleese: 0.5092198252677917\n",
      "pythons: 0.5007115006446838\n",
      "php: 0.4942314028739929\n",
      "grail: 0.4683017134666443\n",
      "scripting: 0.46761268377304077\n",
      "skit: 0.4474538266658783\n",
      "javascript: 0.4312553107738495\n",
      "spamalot: 0.43117913603782654\n"
     ]
    }
   ],
   "source": [
    "if 'python' in glove_model:\n",
    "    similar_words = glove_model.most_similar('python', topn=10)\n",
    "    print(\"Words similar to 'python':\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"{word}: {similarity}\")\n",
    "else:\n",
    "    print(\"Word 'python' is not in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to kurtosis:\n",
      "skewness: 0.6981668472290039\n",
      "cumulant: 0.3952988386154175\n",
      "formula_70: 0.3895203471183777\n",
      "formula_69: 0.38466396927833557\n",
      "alphas: 0.38312193751335144\n",
      "multinomial: 0.3823775351047516\n",
      "k-space: 0.37801066040992737\n",
      "kt/v: 0.3779199421405792\n",
      "vesca: 0.37692299485206604\n",
      "infimum: 0.3755549192428589\n"
     ]
    }
   ],
   "source": [
    "word = \"kurtosis\"\n",
    "\n",
    "if word in glove_model:\n",
    "    similar_words = glove_model.most_similar(word, topn=10)\n",
    "    print(f\"Words similar to {word}:\")\n",
    "    for this, similarity in similar_words:\n",
    "        print(f\"{this}: {similarity}\")\n",
    "else:\n",
    "    print(f\"Word {word} is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a-b-c-d-e-f-g-h-i-j-k-l-m-n-o-p-q-r-s-t-u-v-w-x-y-z-A-B-C-D-E-F-G-H-I-J-K-L-M-N-O-P-Q-R-S-T-U-V-W-X-Y-Z-"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "voc = string.ascii_letters\n",
    "\n",
    "for v in voc:\n",
    "    print(v, end=\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workproject",
   "language": "python",
   "name": "workproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
